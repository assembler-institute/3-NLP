{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0993ac2d",
   "metadata": {},
   "source": [
    "# NLP VI: Conversational Bots and Voice Recognition\n",
    "\n",
    "This notebook aims to arm you with the essential knowledge and practical skills to build your own conversational bot. But we won't stop there! You'll also delve into the captivating realm of voice recognition technology, enabling even more dynamic interactions with your bot.\n",
    "\n",
    "\n",
    "## Introduction to Basic Chat Functionality\n",
    "\n",
    "We'll start by creating a simple chatbot using the NLTK Chat package. This user-friendly library provides a class designed for building simple chatbots, which utilize pattern-matching algorithms to intelligently respond to sentences input by users with contextually appropriate, automatically-generated replies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871ef39b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.chat.util import Chat, reflections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dad67b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "reflections = {\"yo soy\":\"tú eres\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c60a6c",
   "metadata": {},
   "source": [
    "Let us define a tuple containing a set of pattern-response pairs for a simple chatbot.\n",
    "\n",
    "This setup allows the chatbot to have a dynamic, though limited, range of responses depending on the user's input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e008f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = (\n",
    "  (r'Necesito (.*)',\n",
    "   (\"Por qué necesitas %1?\",\n",
    "    \"Realmente te aydar %1?\",\n",
    "    \"Estás seguro de que necesitas %1?\")),\n",
    "    \n",
    "  (r'salir',\n",
    "   (\"Gracias por hablar conmigo.\",\n",
    "    \"Adiós.\",\n",
    "    \"Gracias, que tengas un buen día!\")),\n",
    " \n",
    "  (r'(.*)',\n",
    "   (\"Por favor, cuéntame más detalles.\",\n",
    "    \"Cambiemos de tema... cuentame qué tal tu familia.\",\n",
    "    \"Podrías ser más específico?\",\n",
    "    \"Por qué dices %1?\",\n",
    "    \"Entiendo.\",\n",
    "    \"Muy interesante.\",\n",
    "    \"%1.\",\n",
    "    \"Ya veo.  Y eso qué representa para tí?\",\n",
    "    \"Cómo te hace sentir eso?\",\n",
    "    \"Cómo te sientes cuando dices eso?\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7beb903b",
   "metadata": {},
   "source": [
    "Let us now initialize a new instance of the Chat class, part of the NLTK chat utility. The `pairs` argument is a tuple containing your predefined patterns and responses for the chatbot to use. The `reflections` is a dictionary that contains a set of input text to output text mapping, allowing the chatbot to replace words like \"I am\" with \"you are\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb96a776",
   "metadata": {},
   "outputs": [],
   "source": [
    "chatbot = Chat(pairs, reflections)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660baee0",
   "metadata": {},
   "source": [
    "We create a function for the chatbot entering a loop, waiting for user input and responding based on the pattern-response pairs you've defined. It will also apply any specified reflections to the user input before pattern matching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445d970c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def testChat():\n",
    "    print(\"Bot de prueba\\n\",\"-\"*62)\n",
    "    print('escribe \"salir\" para terminar la conversación.')\n",
    "    print('='*62)\n",
    "    print(\"Hola.  ¿Qué tal estás?\")\n",
    "\n",
    "    chatbot.converse(quit='salir')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f96a8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "testChat()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e60838c",
   "metadata": {},
   "source": [
    "Interacting with the chatbot. The chatbot's mechanism is based on pattern matching. It waits for user input, then searches for a pattern in its predefined pairs that matches the input. Once it finds a match, it generates a response based on the corresponding reply in the pair.\n",
    "\n",
    "The `chatbot.converse()` method does not show the user input as part of its default behavior. You can modify the `testChat()` function to directly call `chatbot.respond()` in order to include both user and bot messages in the output. \n",
    "\n",
    "**Excercice:** Update the previous version of the chatbot code so that the output resembles a natural conversation between the user and the bot like this one:\n",
    "\n",
    "<div style=\"text-align:center;\">\n",
    "<img src=\"Images/ChatBot.png\" width=\"300\">\n",
    "</div>\n",
    "\n",
    "**Hint:** ChatGPT is your friend, optimize your time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b004e193",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Type your code here:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c132413",
   "metadata": {},
   "source": [
    "Ok... your thoughts are correct... \n",
    "\n",
    "It seems like a crazy conversation..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9dca08c",
   "metadata": {},
   "source": [
    "## SPEECH RECOGNITION AND SPEECH SYNTHESIS\n",
    "\n",
    "Speech recognition involves the conversion of spoken language into text (string), whereas speech synthesis accomplishes the reverse—turning text into spoken language (audio).\n",
    "\n",
    "Both processes can be initiated either from a pre-recorded audio file or directly from a device's microphone. Upon capturing the sound source, the audio is then digitized (converted into digital data). Subsequently, decoding based on Markov models is applied to this digital data, resulting in a textual output.\n",
    "\n",
    "This decoding is a complex task that involves slicing the audio into 10-millisecond fragments, mapping each phoneme, and then determining the most likely word sequence. Given the intensive computational resources required to perform these operations, cloud-based services are often employed for this purpose. While it's technically possible to create these Markov models from scratch, the effort, resources, and time needed make it an impractical endeavor.\n",
    "\n",
    "\n",
    "### Requirements\n",
    "\n",
    "In order to follow this notebook correctly we will need to have a microphone in our computer and the following libraries installed:\n",
    "\n",
    "* SpeechRecognition\n",
    "* Pyaudio\n",
    "\n",
    "In case you don't have them installed, here's how to install them.\n",
    "\n",
    "#### SpeechRecognition\n",
    "\n",
    "1. We will open a console of \"Anaconda Promt\".\n",
    "2. We will execute the command: `pip install SpeechRecognition`\n",
    "\n",
    "#### Pyaudio\n",
    "\n",
    "1. We will open a console of \"Anaconda Promt\".\n",
    "2. We will execute the command: `conda install pyaudio`\n",
    "3. Below is this one: `conda install -c anaconda portaudio`\n",
    "4. Finally this one: `pip install pyttsx3`\n",
    "\n",
    "### Speech recognition\n",
    "\n",
    "First we will see how to recognise speech and convert it to text.\n",
    "\n",
    "#### Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e0933f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import speech_recognition as sr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bddef07",
   "metadata": {},
   "source": [
    "### Creating the speech recognition facility\n",
    "\n",
    "To instantiate the speech recognition engine, all we need to do is execute the `Recognizer()` command. This will set up the recognizer object, which you can then use to perform various speech-to-text operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7242879",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize recognizer\n",
    "instance = sr.Recognizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637d2c98",
   "metadata": {},
   "source": [
    "### Selecting the Audio Source\n",
    "\n",
    "As mentioned earlier, you have two options when it comes to selecting the source of the audio: it can either come from a WAV file or be captured directly from your device's microphone. Let's explore both methods.\n",
    "\n",
    "#### Audio from File\n",
    "\n",
    "Obtaining audio from a file is straightforward. All you need to do is load the file using the `AudioFile('FILE_PATH')` command. This will enable the subsequent steps of the speech recognition process to use this file as the audio source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0b4509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load audio file\n",
    "audio_file_path = 'Files/example.wav'\n",
    "\n",
    "file = sr.AudioFile(audio_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06c0874",
   "metadata": {},
   "source": [
    "Here you have a little example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21bfabc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the audio file\n",
    "with sr.AudioFile(audio_file_path) as source:\n",
    "    audio_data = instance.record(source, offset=0, duration=20)\n",
    "    #print(\"Audio Data:\", audio_data)\n",
    "    \n",
    "# Perform speech recognition\n",
    "text = instance.recognize_google(audio_data, language='es-ES')\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a3ba22",
   "metadata": {},
   "source": [
    "#### Capturing Audio from the Microphone\n",
    "\n",
    "To start capturing audio from a device's microphone, we'll use the `Microphone()` function. This function initializes the microphone and prepares it for audio capture, allowing us to record spoken language that can be further processed for speech recognition or other audio analysis tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5697d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In case you get an error when executing the Microphone() command, uncomment these lines\n",
    "# import sys\n",
    "# !{sys.executable} -m pip install pyaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5abb0476",
   "metadata": {},
   "outputs": [],
   "source": [
    "mic = sr.Microphone()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b3975e",
   "metadata": {},
   "source": [
    "The `Microphone()` command will select by default the microphone that our system has by default, but we can select any other microphone installed on the system, even the audio output that we could use to transcribe, for example, a meeting.\n",
    "\n",
    "To see the list of microphones, run the command `list_microphone_names()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3fe8bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sr.Microphone().list_microphone_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710bfe21",
   "metadata": {},
   "source": [
    "To select any other microphone in the list, simply specify the order in the list within the Micrphone() command.\n",
    "\n",
    "For instance, `Microphone(device_index=12)` will select the 12th in the list, in this case \"Microphone (USB Microphone)\".\n",
    "\n",
    "#### Create the audio fragment\n",
    "\n",
    "Once we have the input, either from audio or from the microphone, we set it as the source. In this process is when we would eliminate the background noise with the command `adjust_for_ambient_noise()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e149d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mic_conversion():\n",
    "    with mic as source:\n",
    "        \n",
    "        #Ambient sound settings\n",
    "        instance.adjust_for_ambient_noise(source)\n",
    "        \n",
    "        #Start recording audio\n",
    "        audio = instance.listen(source)\n",
    "        \n",
    "        #Transcribe using google api\n",
    "        transcript = instance.recognize_google(audio, language='es-ES', show_all = True)\n",
    "        \n",
    "        print(transcript)\n",
    "        #We return the result obtained\n",
    "        return transcript ['alternative'][0]['transcript']\n",
    "        \n",
    "def audio_conversion():\n",
    "    with file as source:\n",
    "        \n",
    "        instance.adjust_for_ambient_noise(source)\n",
    "        \n",
    "        audio = instance.record(source)\n",
    "        \n",
    "        transcript = instance.recognize_google(audio, language='es-ES', show_all = True)\n",
    "        \n",
    "        print(transcript)\n",
    "        \n",
    "        return transcript ['alternative'][0]['transcript']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903e5e51",
   "metadata": {},
   "source": [
    "We are now going to test the functions we have created.\n",
    "\n",
    "#### Microphone function\n",
    "\n",
    "When we execute the command, we must wait a second to start speaking to avoid cutting off the beginning of the audio. The function will automatically stop recording after you stop speaking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9106c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mic_conversion()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2bc771f",
   "metadata": {},
   "source": [
    "#### Audio processing function\n",
    "\n",
    "Now we will do the same with the text function. It will not recognise the beginning of the text, so the recognition will not be quite correct.\n",
    "\n",
    "The text contained in the audio is as follows:\n",
    "\n",
    "`La Policía Nacional ha finalizado hoy la implantación de esta nueva versión del DNI y desde hoy únicamente se expedirá este DNI Europeo para todos los ciudadanos españoles.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862be4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_conversion()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca3b49e",
   "metadata": {},
   "source": [
    "As you can see, the first model performed well, while the latter did not. There is likely room for improvement in the second model.\n",
    "\n",
    "It's not uncommon to find that different models yield different levels of performance. If the first model performs well and the second one doesn't, there could be multiple reasons for this discrepancy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3a4058",
   "metadata": {},
   "source": [
    "## Speech synthesis\n",
    "\n",
    "First we will see how to recognise speech and convert it to text.\n",
    "\n",
    "### Importing libraries\n",
    "\n",
    "Use the command: `pip install pyttsx3==2.6`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0314b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyttsx3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b35ae4",
   "metadata": {},
   "source": [
    "### Recognition engine\n",
    "\n",
    "The first thing we have to do is to create the recognition engine, to do this we just have to execute the command `init()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60314957",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = pyttsx3.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fedeb878",
   "metadata": {},
   "source": [
    "### Engine configuration\n",
    "\n",
    "Once it has been created, it is time to configure it. In our case, the configuration will be static, since we will set the language and the speed and we will not need to modify the parameters regularly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da1ea65",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Speed setting\n",
    "engine.setProperty('rate', 140)\n",
    "\n",
    "#Language settings\n",
    "engine.setProperty('voice', 'spanish')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c968963",
   "metadata": {},
   "source": [
    "### Talking function\n",
    "\n",
    "Now we just need to create a function to make our engine talk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1cb7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def habla(texto):\n",
    "    engine.say(texto)\n",
    "    engine.runAndWait()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9546f29",
   "metadata": {},
   "source": [
    "### Testing the function\n",
    "\n",
    "Now all that remains is to test the function we have created and listen to how the text is processed. We will use the same text to compare the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b437573f",
   "metadata": {},
   "outputs": [],
   "source": [
    "texto = 'La Policía Nacional ha finalizado hoy la implantación de esta nueva versión del DNI y desde hoy únicamente se expedirá este DNI Europeo para todos los ciudadanos españoles.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5ce2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "habla(texto)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
